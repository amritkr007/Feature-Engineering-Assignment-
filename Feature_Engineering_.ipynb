{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions"
      ],
      "metadata": {
        "id": "9PH5l9vVBrvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a parameter?\n",
        "- In machine learning (ML), a parameter is a configuration variable that is learned from the training data by the model. These parameters are what the model adjusts during training in order to make accurate predictions."
      ],
      "metadata": {
        "id": "8c489VIsBxqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation?\n",
        "What does negative correlation mean?\n",
        "- Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "- A negative correlation means that as one variable increases, the other decreases."
      ],
      "metadata": {
        "id": "xr1407UvB5-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine Learning (ML) is a branch of artificial intelligence (AI) that allows computers to learn from data and make decisions or predictions without being explicitly programmed.\n",
        "\n",
        "- It’s like teaching a computer to recognize patterns and improve from experience, similar to how humans learn.\n",
        "\n",
        "1. Data\n",
        "This is the raw material for ML.\n",
        "\n",
        "It can be numbers, images, text, audio, etc.\n",
        "\n",
        "The model learns patterns from training data and is tested on testing data.\n",
        "\n",
        "2. Model\n",
        "The mathematical structure that makes predictions or decisions based on the data.\n",
        "\n",
        "Example: linear regression, decision tree, neural networks.\n",
        "\n",
        "3. Algorithm\n",
        "The step-by-step process used to train the model.\n",
        "\n",
        "It adjusts the model's parameters to minimize errors.\n",
        "\n",
        "Examples: Gradient Descent, Random Forest, K-Means."
      ],
      "metadata": {
        "id": "ilFQPJLfCM9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.How does loss value help in determining whether the model is good or not?\n",
        "- Loss is a number that shows how far off a model's predictions are from the actual values.\n",
        "It’s like a score:\n",
        "\n",
        "Lower = Better\n",
        "\n",
        "Higher = Worse\n",
        "\n",
        "- Training Progress\n",
        "\n",
        "  - During training, the model tries to minimize the loss.\n",
        "\n",
        "  - If the loss keeps decreasing, it means the model is learning well.\n",
        "\n",
        "- Model Comparison\n",
        "\n",
        "  - You can use the loss to compare different models or configurations.\n",
        "\n",
        "  - The one with the lowest loss on the validation/test data is usually better.\n",
        "\n",
        "- Detecting Overfitting\n",
        "\n",
        "  -\n",
        "  3\n",
        "  30\n",
        "  \n",
        "  3\n",
        "  .00213.\n",
        "  \n",
        "  If the training loss is low but validation loss is high, your model might be overfitting (memorizing the data instead of generalizing)."
      ],
      "metadata": {
        "id": "CNB9d6o9CksJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are continuous and categorical variables?\n",
        "- These are variables that can take any value within a range—including decimals.\n",
        "\n",
        "✅ Examples:\n",
        "Height (like 5.8 feet)\n",
        "\n",
        "Temperature (like 23.4°C)\n",
        "\n",
        "Salary (like ₹50,000.75)\n",
        "\n",
        "Age (like 25.5 years)\n",
        "\n",
        "These are variables that represent categories or groups. They don't have a mathematical meaning—you can’t calculate averages or differences like you do with numbers.\n",
        "\n",
        "✅ Examples:\n",
        "Gender (Male, Female, Other)\n",
        "\n",
        "City (Delhi, Mumbai, Chennai)\n",
        "\n",
        "Blood Type (A, B, AB, O)\n",
        "\n",
        "Product Type (Shirt, Shoes, Hat)"
      ],
      "metadata": {
        "id": "JchiEhfBDZIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "- In machine learning, models can't directly understand text or labels—they need numbers. So, we have to convert categorical variables into a numerical form. That process is called encoding.\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "Converts categories into numbers (e.g., A = 0, B = 1, C = 2)\n",
        "\n",
        "Simple and fast, but be careful: it implies order, which can confuse some models.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "\n",
        "Creates a new binary column for each category (1 = present, 0 = not present)\n",
        "\n",
        "No order assumed—great for nominal (unordered) data"
      ],
      "metadata": {
        "id": "NE4q-u9hDl3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do you mean by training and testing a dataset?\n",
        "- 1. Training Set\n",
        "Used to train the model.\n",
        "\n",
        "The model learns patterns from this data.\n",
        "\n",
        "It adjusts its internal parameters (like weights in neural networks) based on this data.\n",
        "\n",
        "Think of it like teaching a student using a textbook.\n",
        "\n",
        "2. Testing Set\n",
        "Used to evaluate the model after training.\n",
        "\n",
        "This data is not seen by the model during training.\n",
        "\n",
        "Helps us check how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "brUDMUcMEIpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that provides tools to prepare and transform your data before feeding it into a machine learning model."
      ],
      "metadata": {
        "id": "6oBs3BxMEfxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is a Test set?\n",
        "- A test set is a portion of your dataset that you set aside to evaluate your machine learning model after it has been trained.\n",
        "\n",
        "It’s like the final exam for your model — it hasn’t seen this data before.\n",
        "\n"
      ],
      "metadata": {
        "id": "jkUyzPxzEoeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "- 1. Understand the Problem\n",
        "What are you trying to predict or classify?\n",
        "\n",
        "Is it classification, regression, clustering, etc.?\n",
        "\n",
        "2. Collect the Data\n",
        "Use existing datasets or collect from APIs, files, web scraping, etc.\n",
        "\n",
        "3. Explore the Data (EDA)\n",
        "Use pandas, matplotlib, seaborn\n",
        "\n",
        "Look for patterns, missing values, outliers\n",
        "\n",
        "4. Preprocess the Data\n",
        "Handle missing values\n",
        "\n",
        "Encode categorical variables\n",
        "\n",
        "Scale/normalize features\n",
        "\n",
        "Feature engineering (if needed)\n",
        "\n",
        "5. Split the Data\n",
        "Train-test split (as shown above)\n",
        "\n",
        "Optionally: use a validation set or cross-validation\n",
        "\n",
        "6. Choose a Model\n",
        "Pick a model depending on the task (e.g., LogisticRegression, RandomForest, XGBoost, etc.)"
      ],
      "metadata": {
        "id": "yDUh-2X0FFHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Why do we have to perform EDA before fitting a model to the data?\n",
        "- Exploratory Data Analysis (EDA) is a critical step in the machine learning workflow for several reasons. It allows you to understand and clean your data before feeding it into a model. Here's why it's so important:\n",
        "\n",
        "1. Understand the Data\n",
        "Types of Variables: You need to understand if your features are numerical, categorical, or text-based to choose the correct model and preprocessing steps.\n",
        "\n",
        "Patterns and Relationships: EDA helps reveal potential relationships between features, which can guide feature engineering and model selection.\n",
        "\n",
        "Example: Are features such as age and salary related to the target variable? Does one feature predict the target more strongly than others?\n",
        "\n",
        "2. Identify and Handle Missing Data\n",
        "Many real-world datasets have missing values (e.g., NaNs or nulls). Without addressing this, models may fail or give inaccurate results.\n",
        "\n",
        "EDA helps you spot missing values early, so you can decide to:\n",
        "\n",
        "Impute the missing data\n",
        "\n",
        "Remove rows or columns with missing values\n",
        "\n",
        "Use models that can handle missing values automatically\n",
        "\n",
        "Example: You might find that \"Age\" is missing for 10% of the dataset and decide to fill it with the mean or median age, or perhaps remove rows with missing age if it’s a small percentage.\n",
        "\n",
        "3. Detect Outliers\n",
        "Outliers can skew your results and mislead the model, especially for algorithms like linear regression or k-NN.\n",
        "\n",
        "Through EDA, you can spot extreme values and decide whether to:\n",
        "\n",
        "Remove them if they're errors\n",
        "\n",
        "Use transformations (like log transformations) to minimize their impact\n",
        "\n",
        "Example: A house price dataset might have a few houses priced in the millions that are far beyond typical prices—removing these could improve the model.\n",
        "\n",
        "4. Visualize Distributions and Relationships\n",
        "Visualizations like histograms, scatter plots, and box plots help you understand the distribution of data and relationships between features.\n",
        "\n",
        "Imbalance: In classification tasks, EDA helps you spot class imbalances (e.g., 90% of data points belong to one class).\n",
        "\n",
        "Skewness: If a feature is heavily skewed, you might decide to transform it for better model performance.\n",
        "\n",
        "Example: A feature like \"income\" might be right-skewed (many low-income values and few very high values). You might want to apply a log transformation to make it more normally distributed.\n",
        "\n",
        "5. Feature Engineering\n",
        "EDA can help you create new features (or drop irrelevant ones). For instance, you may discover:\n",
        "\n",
        "Some features have a strong correlation with the target, so they should be used in the model.\n",
        "\n",
        "Some features might be redundant or highly correlated with others, and you might drop one of them to avoid multicollinearity.\n",
        "\n",
        "Example: In a dataset with both \"year\" and \"month\" of purchase, you might combine them into a single \"purchase_time\" feature, or use their interaction to create new features.\n",
        "\n",
        "6. Check Assumptions\n",
        "Some models (e.g., linear regression) assume certain things, like linearity, normality of errors, and no multicollinearity.\n",
        "\n",
        "EDA helps verify these assumptions:\n",
        "\n",
        "Check linear relationships between predictors and the target.\n",
        "\n",
        "Check if features follow a normal distribution (if necessary).\n",
        "\n",
        "Identify correlation between independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-5UazqDFgZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is correlation?\n",
        "- Correlation is a statistical measure that describes the relationship between two or more variables. It indicates how one variable changes in relation to another. In simpler terms, it helps us understand if and how two variables are connected."
      ],
      "metadata": {
        "id": "1LK15aXeGPDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?\n",
        "- Negative correlation refers to a relationship between two variables where, as one variable increases, the other decreases, and vice versa. In other words, when one variable goes up, the other tends to go down."
      ],
      "metadata": {
        "id": "rJBl2BBfHCEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can you find correlation between variables in Python?\n",
        "- Correlation tells you if two things are related.\n",
        "\n",
        "If one thing goes up and the other also goes up, they have a positive correlation.\n",
        "\n",
        "If one thing goes up and the other goes down, they have a negative correlation.\n",
        "\n",
        "If they don't affect each other, their correlation is 0 (no relationship).\n",
        "\n",
        "Using pandas:\n",
        "\n",
        "corr() is the function you use to find how much two things are related (correlated) in a dataset.\n",
        "\n",
        "If you have data in a table (DataFrame), df.corr() will give you a correlation matrix (a table showing how each pair of variables are related).\n",
        "\n",
        "Visualizing with a Heatmap:\n",
        "\n",
        "You can use seaborn to visualize the correlation in a graph, where colors tell you how strong the relationship is.\n",
        "\n"
      ],
      "metadata": {
        "id": "UF_sKCdDHom_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation refers to a cause-and-effect relationship, where one variable directly causes the change in another variable. If A causes B, it means that A is responsible for bringing about a change in B.\n",
        "\n",
        "Key Difference Between Correlation and Causation\n",
        "\n",
        "Correlation tells you that two variables are related (they change together), but it doesn’t tell you that one causes the other.\n",
        "\n",
        "Causation tells you that one variable directly influences or causes the change in another."
      ],
      "metadata": {
        "id": "ci7e24gSI4n_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
        "- An optimizer in machine learning is an algorithm used to minimize or maximize a loss function (also called a cost function or objective function). It adjusts the weights or parameters of a machine learning model during the training process to make the model perform better. The goal of an optimizer is to find the optimal set of parameters that minimize the loss function and improve the model's predictions.\n",
        "\n",
        "Different Types of Optimizers\n",
        "There are several types of optimizers, each with its approach to adjusting the weights or parameters. Here are the most commonly used optimizers:\n",
        "\n",
        "1. Gradient Descent\n",
        "What it is: The most basic optimization algorithm. It uses the gradient (slope) of the loss function to update the model parameters (weights).\n",
        "\n",
        "How it works: Gradient Descent works by calculating the gradient of the loss function with respect to the parameters and moving the parameters in the opposite direction of the gradient to reduce the loss.\n",
        "\n",
        "Variants:\n",
        "\n",
        "Batch Gradient Descent: Uses the entire training dataset to calculate the gradient for each update.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Uses a single data point to update the parameters at each step.\n",
        "\n",
        "Mini-batch Gradient Descent: Uses a subset of the data (mini-batch) to update the parameters.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "What it is: A variant of Gradient Descent where the model parameters are updated based on a single random data point at each iteration, rather than the entire dataset.\n",
        "\n",
        "Advantages: Faster and more memory-efficient since it updates parameters more frequently.\n",
        "\n",
        "Disadvantages: Can be noisy and less stable than Batch Gradient Descent, as the updates are based on individual data points."
      ],
      "metadata": {
        "id": "lqWrgvT_JHQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?\n",
        "- In scikit-learn (sklearn), sklearn.linear_model is a module that contains algorithms for linear modeling techniques. Linear models are used for predicting continuous (regression) or categorical (classification) outcomes based on a linear relationship between the input features and the output."
      ],
      "metadata": {
        "id": "I8BRv9VRJZng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given?\n",
        "- The fit() method in machine learning is used to train a model using a given dataset. It adjusts the model's internal parameters (such as weights for regression models or classifiers) to learn from the data. Essentially, the model fits the data, learning the underlying patterns or relationships between the input data (features) and the output (target variable).\n",
        "\n",
        "Data Input:\n",
        "\n",
        "It takes in your training data, which consists of:\n",
        "\n",
        "Features (X): The input variables that the model will use to make predictions.\n",
        "\n",
        "Target (y): The output or labels (for supervised learning), which the model will try to predict based on the features.\n",
        "\n",
        "Training: During the .fit() process, the model:\n",
        "\n",
        "Learns the relationship between the features (X) and the target (y).\n",
        "\n",
        "Updates its internal parameters (e.g., weights or coefficients in a regression model) using an optimization algorithm (like gradient descent or other algorithms)."
      ],
      "metadata": {
        "id": "foHqoOuZJhzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n",
        "- The model.predict() method is used to make predictions using a trained machine learning model. After you have trained a model using model.fit(), you can use predict() to generate predictions based on new, unseen data (i.e., input data for which you don’t have the target values yet).\n",
        "\n",
        "In other words, model.predict() takes the input features (e.g., data you want to make predictions for) and returns the predicted outputs (labels, values, etc.) based on the patterns the model has learned during training.\n",
        "\n",
        "Input Data (X): You provide the model with a set of input data points, typically represented as a 2D array (matrix) or a 1D array (vector). These are the new data points you want the model to make predictions for.\n",
        "\n",
        "Model Prediction: The model uses the learned relationships (parameters, weights, etc.) from the training process to predict the output for each data point.\n",
        "\n",
        "Return Value: It outputs the predicted values, which could be:\n",
        "\n",
        "For regression: continuous numeric values (e.g., predicted prices, quantities).\n",
        "\n",
        "For classification: predicted class labels (e.g., predicted categories or classes).\n",
        "\n"
      ],
      "metadata": {
        "id": "jLovHGLTJukU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables?\n",
        "- Continuous variables are numerical variables that can take on an infinite number of values within a given range. These values are measured and can be decimal numbers, representing quantities that can be divided into smaller increments. Continuous variables often come from measurements, and they have meaningful intervals between their values.\n",
        "\n",
        "- Categorical variables represent data that can be divided into distinct categories or groups. The values of categorical variables are usually labels or names. Unlike continuous variables, categorical variables can’t be measured or ordered in a meaningful way unless they have a specific ordering (ordinal categories)."
      ],
      "metadata": {
        "id": "N1xjfeypKASD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling is the process of normalizing or standardizing the range of features (input variables) in your dataset so that they have comparable magnitudes. It ensures that no particular feature dominates over others due to differences in their scales or units.\n",
        "\n",
        "- Improves Convergence in Optimization Algorithms: Many machine learning algorithms, especially those that use gradient-based optimization (like linear regression, logistic regression, and neural networks), perform better when the features are scaled. If features have vastly different ranges, the gradient descent algorithm can take longer to converge or might converge to a suboptimal solution.\n",
        "\n",
        "- Equal Contribution of Features: Features with larger numerical ranges can dominate the learning process. For example, in a dataset where one feature ranges from 1 to 1000 and another ranges from 0 to 1, the model may give more importance to the first feature unless both features are scaled properly.\n",
        "\n",
        "- Improves Accuracy of Certain Algorithms: Algorithms that are distance-based, like k-NN, SVM, and Principal Component Analysis (PCA), are sensitive to the scale of the data. If one feature has a much larger range than others, it will have a disproportionately large effect on the distance between data points, which can skew the model's predictions."
      ],
      "metadata": {
        "id": "zH5vP07uKMJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python?\n",
        "- In Python, scaling can be easily performed using the sklearn.preprocessing module, which provides various methods for both normalization and standardization. Below are the steps for performing feature scaling in Python:\n",
        "\n",
        "1.Normalization (Min-Max Scaling)\n",
        "\n",
        "This method scales the features to a fixed range, usually [0, 1].\n",
        "\n",
        "Steps:\n",
        "Import the MinMaxScaler class from sklearn.preprocessing.\n",
        "\n",
        "Instantiate the scaler.\n",
        "\n",
        "Use the fit_transform() method to normalize the data.\n",
        "\n",
        "2.Standardization (Z-score Scaling)\n",
        "\n",
        "This method scales the features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Steps:\n",
        "Import the StandardScaler class from sklearn.preprocessing.\n",
        "\n",
        "Instantiate the scaler.\n",
        "\n",
        "Use the fit_transform() method to standardize the data."
      ],
      "metadata": {
        "id": "tDkiM59MKb28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in Scikit-learn (a Python library for machine learning) that provides tools for data preprocessing, which is an essential step in building machine learning models. It includes a variety of functions and classes to scale, transform, and encode data in a format that can be used effectively by machine learning algorithms.\n",
        "\n",
        "- Preprocessing ensures that the data fed into machine learning models is clean, well-structured, and in a form that the model can interpret correctly. Many machine learning algorithms, such as logistic regression, k-NN, and SVM, perform better when the data is scaled or encoded appropriately."
      ],
      "metadata": {
        "id": "816MJAfdKv2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python?\n",
        "- In machine learning, splitting data into training and testing sets is crucial for evaluating model performance. This ensures that the model is tested on data it hasn't seen before, giving you a sense of how well it will generalize to new, unseen data.\n",
        "\n",
        "In Python, you can easily split your data using the train_test_split function from Scikit-learn's model_selection module.\n",
        "\n",
        "Import Libraries:\n",
        "\n",
        "Import the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "Prepare Your Dataset:\n",
        "\n",
        "You need a dataset (typically features X and target labels y).\n",
        "\n",
        "Split the Data:\n",
        "\n",
        "Use train_test_split to split the dataset into a training set and a testing set.\n",
        "\n",
        "Specify Test Size:\n",
        "\n",
        "You can control the proportion of the dataset to use for training and testing by setting the test_size parameter (commonly 0.2 for an 80-20 split).\n",
        "\n",
        "Shuffling (Optional but Recommended):\n",
        "\n",
        "By default, train_test_split shuffles the data, ensuring the split is random.\n",
        "\n"
      ],
      "metadata": {
        "id": "OFWioE5sK4fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Explain data encoding?\n",
        "- Data encoding refers to the process of converting categorical data (non-numeric data, such as labels or categories) into a numerical format that machine learning algorithms can process. Most machine learning algorithms require numerical input to perform calculations, so encoding is an essential step in preparing data for model training."
      ],
      "metadata": {
        "id": "s2TqyetRLKv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p8wnM09QLUAn"
      }
    }
  ]
}